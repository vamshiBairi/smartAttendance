{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vamshi\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "loaded\n",
      "Loaded Done\n",
      "['shreyesh', 'vamshi']\n",
      "frame recorded\n",
      "Unknown  Is identified\n",
      "frame recorded\n",
      "Unknown  Is identified\n",
      "frame recorded\n",
      "Unknown  Is identified\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 101\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# print(known_faces)\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(known_names)\n\u001b[1;32m--> 101\u001b[0m \u001b[43mrecognize_faces_from_live_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknown_faces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknown_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 51\u001b[0m, in \u001b[0;36mrecognize_faces_from_live_video\u001b[1;34m(known_faces, known_names)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Detect faces with RetinaFace\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mRetinaFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe recorded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Prepare face encodings for recognition\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\retinaface\\RetinaFace.py:123\u001b[0m, in \u001b[0;36mdetect_faces\u001b[1;34m(img_path, threshold, model, allow_upscaling)\u001b[0m\n\u001b[0;32m    121\u001b[0m landmarks_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    122\u001b[0m im_tensor, im_info, im_scale \u001b[38;5;241m=\u001b[39m preprocess\u001b[38;5;241m.\u001b[39mpreprocess_image(img, allow_upscaling)\n\u001b[1;32m--> 123\u001b[0m net_out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m net_out \u001b[38;5;241m=\u001b[39m [elt\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m elt \u001b[38;5;129;01min\u001b[39;00m net_out]\n\u001b[0;32m    125\u001b[0m sym_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from retinaface import RetinaFace\n",
    "import face_recognition\n",
    "import os\n",
    "\n",
    "# Directory containing known faces\n",
    "KNOWN_FACES_DIR = r\"C:\\Users\\vamshi\\OneDrive\\Desktop\\retina\\Training_images\"\n",
    "TOLERANCE = 0.6  # Recognition tolerance\n",
    "FRAME_THICKNESS = 3  # Thickness of bounding box\n",
    "FONT_THICKNESS = 2  # Thickness of text\n",
    "MODEL = \"cnn\"  # The model can be \"hog\" or \"cnn\" for detection\n",
    "\n",
    "# Load known faces and their encodings\n",
    "def load_known_faces():\n",
    "    known_faces = []\n",
    "    known_names = []\n",
    "\n",
    "    for name in os.listdir(KNOWN_FACES_DIR):\n",
    "        for filename in os.listdir(f\"{KNOWN_FACES_DIR}\\{name}\"):\n",
    "            image = face_recognition.load_image_file(f\"{KNOWN_FACES_DIR}\\{name}\\{filename}\")\n",
    "            encodings = face_recognition.face_encodings(image)\n",
    "            if encodings:\n",
    "                known_faces.append(encodings[0])\n",
    "                known_names.append(name)\n",
    "    print(\"loaded\")\n",
    "    return known_faces, known_names\n",
    "\n",
    "# Draw bounding boxes and names\n",
    "def draw_faces(frame, faces, names):\n",
    "    for (face, name) in zip(faces, names):\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (face[0], face[1]), (face[2], face[3]), (0, 255, 0), FRAME_THICKNESS)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        y_position = face[1] - 15 if face[1] - 15 > 15 else face[1] + 15\n",
    "        cv2.rectangle(frame, (face[0], face[1]), (face[2], face[1] - 35), (0, 255, 0), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (face[0] + 6, y_position), font, 0.8, (255, 255, 255), FONT_THICKNESS)\n",
    "\n",
    "# Recognize faces from live video\n",
    "def recognize_faces_from_live_video(known_faces, known_names):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Detect faces with RetinaFace\n",
    "        faces = RetinaFace.detect_faces(frame)\n",
    "        print(\"frame recorded\")\n",
    "        # Prepare face encodings for recognition\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        face_locations = []\n",
    "        face_names = []\n",
    "\n",
    "        for face_key in faces.keys():\n",
    "            facial_area = faces[face_key][\"facial_area\"]\n",
    "\n",
    "            # Append detected face locations for recognition\n",
    "            face_locations.append(facial_area)\n",
    "\n",
    "            # Convert RetinaFace coordinates to dlib format\n",
    "            top, right, bottom, left = facial_area[1], facial_area[2], facial_area[3], facial_area[0]\n",
    "            # Encode the face using face_recognition\n",
    "            encodings = face_recognition.face_encodings(frame_rgb, [(top, right, bottom, left)])\n",
    "\n",
    "            for encoding in encodings:\n",
    "                # Compare with known faces\n",
    "                matches = face_recognition.compare_faces(known_faces, encoding, TOLERANCE)\n",
    "                name = \"Unknown\"\n",
    "\n",
    "                # Find the shortest distance to a known face\n",
    "                face_distances = face_recognition.face_distance(known_faces, encoding)\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_names[best_match_index]\n",
    "                print(name,\" Is identified\")\n",
    "                face_names.append(name)\n",
    "\n",
    "        # Draw the results\n",
    "        draw_faces(frame, face_locations, face_names)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Live Video', frame)\n",
    "        \n",
    "        # Press 'q' to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load known faces\n",
    "    known_faces, known_names = load_known_faces()\n",
    "    print(\"Loaded Done\")\n",
    "    # print(known_faces)\n",
    "    print(known_names)\n",
    "    recognize_faces_from_live_video(known_faces, known_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded known faces\n",
      "Loaded Done\n",
      "['shreyesh', 'vamshi']\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from retinaface import RetinaFace\n",
    "import face_recognition\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import threading\n",
    "KNOWN_FACES_DIR = r\"C:\\Users\\vamshi\\OneDrive\\Desktop\\retina\\Training_images\"\n",
    "TOLERANCE = 0.6  # Recognition tolerance\n",
    "FRAME_THICKNESS = 3  # Thickness of bounding box\n",
    "FONT_THICKNESS = 2  # Thickness of text\n",
    "MODEL = \"cnn\"  # The model can be \"hog\" or \"cnn\" for detection\n",
    "def load_known_faces():\n",
    "    known_faces = []\n",
    "    known_names = []\n",
    "    for name in os.listdir(KNOWN_FACES_DIR):\n",
    "        for filename in os.listdir(f\"{KNOWN_FACES_DIR}/{name}\"):\n",
    "            image = face_recognition.load_image_file(f\"{KNOWN_FACES_DIR}/{name}/{filename}\")\n",
    "            encodings = face_recognition.face_encodings(image)\n",
    "            if encodings:\n",
    "                known_faces.append(encodings[0])\n",
    "                known_names.append(name)\n",
    "    print(\"Loaded known faces\")\n",
    "    return known_faces, known_names\n",
    "def draw_faces(frame, faces, names):\n",
    "    for (face, name) in zip(faces, names):\n",
    "        cv2.rectangle(frame, (face[0], face[1]), (face[2], face[3]), (0, 255, 0), FRAME_THICKNESS)\n",
    "        y_position = face[1] - 15 if face[1] - 15 > 15 else face[1] + 15\n",
    "        cv2.rectangle(frame, (face[0], face[1]), (face[2], face[1] - 35), (0, 255, 0), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (face[0] + 6, y_position), font, 0.8, (255, 255, 255), FONT_THICKNESS)\n",
    "        \n",
    "def recognize_faces_from_camera(camera_id, known_faces, known_names, tracking_dict):\n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        faces = RetinaFace.detect_faces(frame)\n",
    "        print(f\"Frame recorded from Camera {camera_id}\")\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        face_locations = []\n",
    "        face_names = []\n",
    "        current_faces = set() \n",
    "        for face_key in faces.keys():\n",
    "            facial_area = faces[face_key][\"facial_area\"]\n",
    "            face_locations.append(facial_area)\n",
    "            current_faces.add(facial_area)\n",
    "            top, right, bottom, left = facial_area[1], facial_area[2], facial_area[3], facial_area[0]\n",
    "            encodings = face_recognition.face_encodings(frame_rgb, [(top, right, bottom, left)])\n",
    "            for encoding in encodings:\n",
    "                matches = face_recognition.compare_faces(known_faces, encoding, TOLERANCE)\n",
    "                name = \"Unknown\"\n",
    "                face_distances = face_recognition.face_distance(known_faces, encoding)\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_names[best_match_index]\n",
    "                face_names.append(name)\n",
    "                if name not in tracking_dict['seen']:\n",
    "                    if camera_id == 0:\n",
    "                        tracking_dict['entry_writer'].writerow([name, datetime.now().strftime('%Y-%m-%d %H:%M:%S')])\n",
    "                        tracking_dict['seen'][name] = datetime.now()\n",
    "                        tracking_dict['exit_seen'][name] = False\n",
    "                    elif camera_id == 1:\n",
    "                        if not tracking_dict['exit_seen'].get(name, False):\n",
    "                            tracking_dict['exit_writer'].writerow([name, datetime.now().strftime('%Y-%m-%d %H:%M:%S')])\n",
    "                            tracking_dict['exit_seen'][name] = True\n",
    "        if camera_id == 0:\n",
    "            absent_names = set(tracking_dict['seen'].keys()) - set(face_names)\n",
    "            for name in absent_names:\n",
    "                if not tracking_dict['exit_seen'][name]:\n",
    "                    if datetime.now() - tracking_dict['seen'][name] >= timedelta(minutes=2):\n",
    "                        tracking_dict['exit_writer'].writerow([name, datetime.now().strftime('%Y-%m-%d %H:%M:%S')])\n",
    "                        tracking_dict['exit_seen'][name] = True\n",
    "        draw_faces(frame, face_locations, face_names)\n",
    "        cv2.imshow(f'Camera {camera_id}', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "def main():\n",
    "    known_faces, known_names = load_known_faces()\n",
    "    print(\"Loaded Done\")\n",
    "    print(known_names)\n",
    "    with open(r'C:\\Users\\vamshi\\OneDrive\\Desktop\\retina\\attendance.csv', mode='a', newline='') as file:\n",
    "        entry_writer = csv.writer(file)\n",
    "        exit_writer = csv.writer(file)\n",
    "        tracking_dict = {\n",
    "            'entry_writer': entry_writer,\n",
    "            'exit_writer': exit_writer,\n",
    "            'seen': {},  \n",
    "            'exit_seen': {} \n",
    "        }\n",
    "        camera_ids = [0, 1]  \n",
    "        threads = []\n",
    "        for camera_id in camera_ids:\n",
    "            thread = threading.Thread(target=recognize_faces_from_camera, args=(camera_id, known_faces, known_names, tracking_dict))\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from retinaface import RetinaFace\n",
    "import face_recognition\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Directory containing known faces\n",
    "KNOWN_FACES_DIR = r\"C:\\attendance2\\images\"\n",
    "TOLERANCE = 0.6  # Recognition tolerance\n",
    "FRAME_THICKNESS = 3  # Thickness of bounding box\n",
    "FONT_THICKNESS = 2  # Thickness of text\n",
    "MODEL = \"cnn\"  # The model can be \"hog\" or \"cnn\" for detection\n",
    "\n",
    "# Load known faces and their encodings\n",
    "def load_known_faces():\n",
    "    known_faces = []\n",
    "    known_names = []\n",
    "\n",
    "    for name in os.listdir(KNOWN_FACES_DIR):\n",
    "        for filename in os.listdir(f\"{KNOWN_FACES_DIR}/{name}\"):\n",
    "            image = face_recognition.load_image_file(f\"{KNOWN_FACES_DIR}/{name}/{filename}\")\n",
    "            encodings = face_recognition.face_encodings(image)\n",
    "            if encodings:\n",
    "                known_faces.append(encodings[0])\n",
    "                known_names.append(name)\n",
    "    print(\"loaded\")\n",
    "    return known_faces, known_names\n",
    "\n",
    "# Draw bounding boxes and names\n",
    "def draw_faces(frame, faces, names):\n",
    "    for (face, name) in zip(faces, names):\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (face[0], face[1]), (face[2], face[3]), (0, 255, 0), FRAME_THICKNESS)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        y_position = face[1] - 15 if face[1] - 15 > 15 else face[1] + 15\n",
    "        cv2.rectangle(frame, (face[0], face[1]), (face[2], face[1] - 35), (0, 255, 0), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (face[0] + 6, y_position), font, 0.8, (255, 255, 255), FONT_THICKNESS)\n",
    "\n",
    "# Recognize faces from live video and log to CSV\n",
    "def recognize_faces_from_live_video(known_faces, known_names):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Open a CSV file for writing\n",
    "    with open('C:/attendance2/attendance_log.csv', mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Name', 'Timestamp'])  # Header row\n",
    "        \n",
    "        last_record_times = {}  # Dictionary to track last record time for each person\n",
    "        frame_count = 0\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Process every nth frame to reduce the load\n",
    "            frame_count += 1\n",
    "            if frame_count % 5 != 0:  # Change the modulo value to adjust the frame rate\n",
    "                continue\n",
    "\n",
    "            # Detect faces with RetinaFace\n",
    "            faces = RetinaFace.detect_faces(frame)\n",
    "            print(\"frame recorded\")\n",
    "            # Prepare face encodings for recognition\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            face_locations = []\n",
    "            face_names = []\n",
    "\n",
    "            for face_key in faces.keys():\n",
    "                facial_area = faces[face_key][\"facial_area\"]\n",
    "\n",
    "                # Append detected face locations for recognition\n",
    "                face_locations.append(facial_area)\n",
    "\n",
    "                # Convert RetinaFace coordinates to dlib format\n",
    "                top, right, bottom, left = facial_area[1], facial_area[2], facial_area[3], facial_area[0]\n",
    "                # Encode the face using face_recognition\n",
    "                encodings = face_recognition.face_encodings(frame_rgb, [(top, right, bottom, left)])\n",
    "\n",
    "                for encoding in encodings:\n",
    "                    # Compare with known faces\n",
    "                    matches = face_recognition.compare_faces(known_faces, encoding, TOLERANCE)\n",
    "                    name = \"Unknown\"\n",
    "\n",
    "                    # Find the shortest distance to a known face\n",
    "                    face_distances = face_recognition.face_distance(known_faces, encoding)\n",
    "                    best_match_index = np.argmin(face_distances)\n",
    "                    if matches[best_match_index]:\n",
    "                        name = known_names[best_match_index]\n",
    "                    print(name, \"is identified\")\n",
    "                    face_names.append(name)\n",
    "\n",
    "            # Log detected names and timestamps if 2 minutes have passed\n",
    "            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            for name in face_names:\n",
    "                if name != \"Unknown\":\n",
    "                    if name not in last_record_times or (datetime.now() - last_record_times[name]) >= timedelta(minutes=2):\n",
    "                        writer.writerow([name, timestamp])\n",
    "                        last_record_times[name] = datetime.now()  # Update the last record time for this person\n",
    "\n",
    "            # Draw the results\n",
    "            draw_faces(frame, face_locations, face_names)\n",
    "\n",
    "            # Display the frame\n",
    "            cv2.imshow('Live Video', frame)\n",
    "            \n",
    "            # Press 'q' to exit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load known faces\n",
    "    known_faces, known_names = load_known_faces()\n",
    "    print(\"Loaded Done\")\n",
    "    print(known_names)\n",
    "    recognize_faces_from_live_video(known_faces, known_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vamshi\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Loaded known faces.\n",
      "Updated attendance for vamshi at 2024-10-30 13:34:32 for Period 4\n",
      "Entry detected for vamshi at 2024-10-30 13:34:32\n",
      "Updated attendance for vamshi at 2024-10-30 13:35:00 for Period 4\n",
      "Entry detected for vamshi at 2024-10-30 13:35:00\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from retinaface import RetinaFace\n",
    "import face_recognition\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Directory containing known faces\n",
    "KNOWN_FACES_DIR = r\"C:\\Users\\vamshi\\OneDrive\\Desktop\\retina\\Training_images\"\n",
    "TOLERANCE = 0.6\n",
    "FRAME_THICKNESS = 3\n",
    "FONT_THICKNESS = 2\n",
    "MODEL = \"cnn\"\n",
    "\n",
    "# Constants for periods and attendance calculation\n",
    "PERIOD_TIMES = [\n",
    "    (9, 10),   # Period 1: 9-10 am\n",
    "    (10, 11),  # Period 2: 10-11 am\n",
    "    (11, 12),  # Period 3: 11-12 pm\n",
    "    (13, 14),  # Period 4: 1-2 pm\n",
    "    (14, 15),  # Period 5: 2-3 pm\n",
    "    (15, 16)   # Period 6: 3-4 pm\n",
    "]\n",
    "TOTAL_PERIODS = 6\n",
    "\n",
    "# Load known faces and their encodings\n",
    "def load_known_faces():\n",
    "    known_faces = []\n",
    "    known_names = []\n",
    "\n",
    "    for name in os.listdir(KNOWN_FACES_DIR):\n",
    "        for filename in os.listdir(f\"{KNOWN_FACES_DIR}/{name}\"):\n",
    "            image = face_recognition.load_image_file(f\"{KNOWN_FACES_DIR}/{name}/{filename}\")\n",
    "            encodings = face_recognition.face_encodings(image)\n",
    "            if encodings:\n",
    "                known_faces.append(encodings[0])\n",
    "                known_names.append(name)\n",
    "    print(\"Loaded known faces.\")\n",
    "    return known_faces, known_names\n",
    "\n",
    "# Initialize or update attendance file with columns for each period and Total and Percentage\n",
    "def initialize_attendance_file(known_names):\n",
    "    attendance_file = r'C:\\Users\\vamshi\\OneDrive\\Desktop\\retina\\Attend.xlsx'\n",
    "    if not os.path.exists(attendance_file):\n",
    "        columns = ['Name'] + [f'Period {i+1}' for i in range(TOTAL_PERIODS)] + ['Total', 'Percentage']\n",
    "        attendance_data = pd.DataFrame(columns=columns)\n",
    "        attendance_data['Name'] = known_names\n",
    "        attendance_data.fillna(0, inplace=True)  # Set initial attendance to 0\n",
    "        attendance_data.to_excel(attendance_file, index=False)\n",
    "    else:\n",
    "        attendance_data = pd.read_excel(attendance_file, engine='openpyxl')\n",
    "    return attendance_data\n",
    "\n",
    "# Determine current period based on the current time\n",
    "def get_period_index(current_time):\n",
    "    for i, (start, end) in enumerate(PERIOD_TIMES):\n",
    "        if start <= current_time.hour < end:\n",
    "            return i  # Return the period index (0 for Period 1, etc.)\n",
    "    return None  # Return None if outside period times\n",
    "\n",
    "# Update attendance for each period based on entry time\n",
    "def update_attendance(name, timestamp, attendance_data):\n",
    "    attendance_file = r'C:\\Users\\vamshi\\OneDrive\\Desktop\\retina\\Attend.xlsx'\n",
    "    current_time = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "    period_index = get_period_index(current_time)\n",
    "    \n",
    "    if period_index is not None:\n",
    "        period_col = f'Period {period_index + 1}'\n",
    "        \n",
    "        # Increment the attendance count for the detected period\n",
    "        attendance_data.loc[attendance_data['Name'] == name, period_col] += 1\n",
    "        \n",
    "        # Update total and recalculate attendance percentage\n",
    "        attendance_data['Total'] = attendance_data[[f'Period {i+1}' for i in range(TOTAL_PERIODS)]].sum(axis=1)\n",
    "        prev_percentage = attendance_data.loc[attendance_data['Name'] == name, 'Percentage'].iloc[0]\n",
    "        classes_today = attendance_data.loc[attendance_data['Name'] == name, [f'Period {i+1}' for i in range(TOTAL_PERIODS)]].sum()\n",
    "        new_percentage = (prev_percentage + (classes_today / TOTAL_PERIODS)) / 2\n",
    "        attendance_data.loc[attendance_data['Name'] == name, 'Percentage'] = new_percentage\n",
    "        \n",
    "        # Save updated attendance data to Excel\n",
    "        attendance_data.to_excel(attendance_file, index=False)\n",
    "        print(f\"Updated attendance for {name} at {timestamp} for {period_col}\")\n",
    "\n",
    "# Recognize faces from entry and exit cameras and process attendance\n",
    "def recognize_faces_multi_camera(known_faces, known_names):\n",
    "    cap_entry = cv2.VideoCapture(0)  # Entry camera\n",
    "    cap_exit = cv2.VideoCapture(1)   # Exit camera\n",
    "\n",
    "    attendance_data = initialize_attendance_file(known_names)\n",
    "    global last_entry_times\n",
    "    last_entry_times = {}  # Dictionary to track last entry times for each person\n",
    "\n",
    "    while True:\n",
    "        # Read frames from both entry and exit cameras\n",
    "        ret_entry, frame_entry = cap_entry.read()\n",
    "        ret_exit, frame_exit = cap_exit.read()\n",
    "        if not ret_entry or not ret_exit:\n",
    "            break\n",
    "\n",
    "        # Process entry camera\n",
    "        process_frame(frame_entry, 'entry', known_faces, known_names, attendance_data)\n",
    "\n",
    "        # Process exit camera\n",
    "        process_frame(frame_exit, 'exit', known_faces, known_names, attendance_data)\n",
    "\n",
    "        # Display frames for entry and exit cameras\n",
    "        cv2.imshow('Entry Camera', frame_entry)\n",
    "        cv2.imshow('Exit Camera', frame_exit)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap_entry.release()\n",
    "    cap_exit.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Process frames for entry or exit events\n",
    "def process_frame(frame, event_type, known_faces, known_names, attendance_data):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    faces = RetinaFace.detect_faces(frame)\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "\n",
    "    for face_key in faces.keys():\n",
    "        facial_area = faces[face_key][\"facial_area\"]\n",
    "        top, right, bottom, left = facial_area[1], facial_area[2], facial_area[3], facial_area[0]\n",
    "        encodings = face_recognition.face_encodings(frame_rgb, [(top, right, bottom, left)])\n",
    "\n",
    "        for encoding in encodings:\n",
    "            matches = face_recognition.compare_faces(known_faces, encoding, TOLERANCE)\n",
    "            name = \"Unknown\"\n",
    "            face_distances = face_recognition.face_distance(known_faces, encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            if matches[best_match_index]:\n",
    "                name = known_names[best_match_index]\n",
    "            face_names.append(name)\n",
    "\n",
    "    # Log detected names and timestamps\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    for name in face_names:\n",
    "        if name != \"Unknown\":\n",
    "            update_attendance(name, timestamp, attendance_data)\n",
    "            print(f\"{event_type.capitalize()} detected for {name} at {timestamp}\")\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    known_faces, known_names = load_known_faces()\n",
    "    recognize_faces_multi_camera(known_faces, known_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vamshi\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Loaded known faces.\n",
      "Entry detected for vamshi at 2024-10-30 14:39:29\n",
      "Updated attendance for vamshi for Period 5 (duration: 2.85 mins)\n",
      "Exit detected for vamshi at 2024-10-30 14:42:20\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from retinaface import RetinaFace\n",
    "import face_recognition\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Directory containing known faces\n",
    "KNOWN_FACES_DIR = r\"C:\\Users\\vamshi\\OneDrive\\Desktop\\retina\\Training_images\"\n",
    "TOLERANCE = 0.6\n",
    "FRAME_THICKNESS = 3\n",
    "FONT_THICKNESS = 2\n",
    "MODEL = \"cnn\"\n",
    "\n",
    "# Constants for periods and attendance calculation\n",
    "PERIOD_TIMES = [\n",
    "    (9, 10),   # Period 1: 9-10 am\n",
    "    (10, 11),  # Period 2: 10-11 am\n",
    "    (11, 12),  # Period 3: 11-12 pm\n",
    "    (13, 14),  # Period 4: 1-2 pm\n",
    "    (14, 15),  # Period 5: 2-3 pm\n",
    "    (15, 16)   # Period 6: 3-4 pm\n",
    "]\n",
    "TOTAL_PERIODS = 6\n",
    "MINIMUM_DURATION = 2  # in minutes to log attendance for the period\n",
    "\n",
    "# Load known faces and their encodings\n",
    "def load_known_faces():\n",
    "    known_faces = []\n",
    "    known_names = []\n",
    "\n",
    "    for name in os.listdir(KNOWN_FACES_DIR):\n",
    "        for filename in os.listdir(f\"{KNOWN_FACES_DIR}/{name}\"):\n",
    "            image = face_recognition.load_image_file(f\"{KNOWN_FACES_DIR}/{name}/{filename}\")\n",
    "            encodings = face_recognition.face_encodings(image)\n",
    "            if encodings:\n",
    "                known_faces.append(encodings[0])\n",
    "                known_names.append(name)\n",
    "    print(\"Loaded known faces.\")\n",
    "    return known_faces, known_names\n",
    "\n",
    "# Initialize or update attendance file with columns for each period and Total and Percentage\n",
    "def initialize_attendance_file(known_names):\n",
    "    attendance_file = r'C:\\Users\\vamshi\\OneDrive\\Desktop\\retina\\Attend.xlsx'\n",
    "    if not os.path.exists(attendance_file):\n",
    "        columns = ['Name'] + [f'Period {i+1}' for i in range(TOTAL_PERIODS)] + ['Total', 'Percentage']\n",
    "        attendance_data = pd.DataFrame(columns=columns)\n",
    "        attendance_data['Name'] = known_names\n",
    "        attendance_data.fillna(0, inplace=True)  # Set initial attendance to 0\n",
    "        attendance_data.to_excel(attendance_file, index=False)\n",
    "    else:\n",
    "        attendance_data = pd.read_excel(attendance_file, engine='openpyxl')\n",
    "    return attendance_data\n",
    "\n",
    "# Determine current period based on the current time\n",
    "def get_period_index(current_time):\n",
    "    for i, (start, end) in enumerate(PERIOD_TIMES):\n",
    "        if start <= current_time.hour < end:\n",
    "            return i  # Return the period index (0 for Period 1, etc.)\n",
    "    return None  # Return None if outside period times\n",
    "\n",
    "# Update attendance for each period based on entry and exit times\n",
    "def update_attendance(name, timestamp, event_type, attendance_data, last_entry_times):\n",
    "    attendance_file = r'C:\\Users\\vamshi\\OneDrive\\Desktop\\retina\\Attend.xlsx'\n",
    "    current_time = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "    period_index = get_period_index(current_time)\n",
    "    \n",
    "    if period_index is not None:\n",
    "        period_col = f'Period {period_index + 1}'\n",
    "        \n",
    "        if event_type == 'entry':\n",
    "            last_entry_times[name] = current_time  # Store entry time\n",
    "        elif event_type == 'exit' and name in last_entry_times:\n",
    "            entry_time = last_entry_times.pop(name, None)  # Retrieve and remove the entry time\n",
    "            duration = (current_time - entry_time).total_seconds() / 60  # Convert to minutes\n",
    "\n",
    "            # Log attendance only if the duration is at least 45 minutes\n",
    "            if duration >= MINIMUM_DURATION:\n",
    "                attendance_data.loc[attendance_data['Name'] == name, period_col] += 1\n",
    "                \n",
    "                # Update total and recalculate attendance percentage\n",
    "                attendance_data['Total'] = attendance_data[[f'Period {i+1}' for i in range(TOTAL_PERIODS)]].sum(axis=1)\n",
    "                prev_percentage = attendance_data.loc[attendance_data['Name'] == name, 'Percentage'].iloc[0]\n",
    "                classes_today = attendance_data.loc[attendance_data['Name'] == name, [f'Period {i+1}' for i in range(TOTAL_PERIODS)]].sum()\n",
    "                new_percentage = (prev_percentage + (classes_today / TOTAL_PERIODS)) / 2\n",
    "                attendance_data.loc[attendance_data['Name'] == name, 'Percentage'] = new_percentage\n",
    "                \n",
    "                # Save updated attendance data to Excel\n",
    "                attendance_data.to_excel(attendance_file, index=False)\n",
    "                print(f\"Updated attendance for {name} for {period_col} (duration: {duration} mins)\")\n",
    "\n",
    "# Recognize faces from entry and exit cameras and process attendance\n",
    "def recognize_faces_multi_camera(known_faces, known_names):\n",
    "    cap_entry = cv2.VideoCapture(0)  # Entry camera\n",
    "    cap_exit = cv2.VideoCapture(1)   # Exit camera\n",
    "\n",
    "    attendance_data = initialize_attendance_file(known_names)\n",
    "    last_entry_times = {}  # Dictionary to track last entry times for each person\n",
    "\n",
    "    while True:\n",
    "        # Read frames from both entry and exit cameras\n",
    "        ret_entry, frame_entry = cap_entry.read()\n",
    "        ret_exit, frame_exit = cap_exit.read()\n",
    "        if not ret_entry or not ret_exit:\n",
    "            break\n",
    "\n",
    "        # Process entry camera\n",
    "        process_frame(frame_entry, 'entry', known_faces, known_names, attendance_data, last_entry_times)\n",
    "\n",
    "        # Process exit camera\n",
    "        process_frame(frame_exit, 'exit', known_faces, known_names, attendance_data, last_entry_times)\n",
    "\n",
    "        # Display frames for entry and exit cameras\n",
    "        cv2.imshow('Entry Camera', frame_entry)\n",
    "        cv2.imshow('Exit Camera', frame_exit)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap_entry.release()\n",
    "    cap_exit.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Process frames for entry or exit events\n",
    "def process_frame(frame, event_type, known_faces, known_names, attendance_data, last_entry_times):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    faces = RetinaFace.detect_faces(frame)\n",
    "    face_names = []\n",
    "\n",
    "    for face_key in faces.keys():\n",
    "        facial_area = faces[face_key][\"facial_area\"]\n",
    "        top, right, bottom, left = facial_area[1], facial_area[2], facial_area[3], facial_area[0]\n",
    "        encodings = face_recognition.face_encodings(frame_rgb, [(top, right, bottom, left)])\n",
    "\n",
    "        for encoding in encodings:\n",
    "            matches = face_recognition.compare_faces(known_faces, encoding, TOLERANCE)\n",
    "            name = \"Unknown\"\n",
    "            face_distances = face_recognition.face_distance(known_faces, encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            if matches[best_match_index]:\n",
    "                name = known_names[best_match_index]\n",
    "            face_names.append(name)\n",
    "\n",
    "    # Log detected names and timestamps\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    for name in face_names:\n",
    "        if name != \"Unknown\":\n",
    "            update_attendance(name, timestamp, event_type, attendance_data, last_entry_times)\n",
    "            print(f\"{event_type.capitalize()} detected for {name} at {timestamp}\")\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    known_faces, known_names = load_known_faces()\n",
    "    recognize_faces_multi_camera(known_faces, known_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vamshi\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from retinaface import RetinaFace\n",
    "import face_recognition\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Directory containing known faces\n",
    "KNOWN_FACES_DIR = r\"C:\\Users\\vamshi\\OneDrive\\Desktop\\retina\\Training_images\"\n",
    "TOLERANCE = 0.6\n",
    "FRAME_THICKNESS = 3\n",
    "FONT_THICKNESS = 2\n",
    "MODEL = \"cnn\"\n",
    "\n",
    "# Constants for periods and attendance calculation\n",
    "PERIOD_TIMES = [\n",
    "    (9, 10),   # Period 1: 9-10 am\n",
    "    (10, 11),  # Period 2: 10-11 am\n",
    "    (11, 12),  # Period 3: 11-12 pm\n",
    "    (13, 14),  # Period 4: 1-2 pm\n",
    "    (14, 15),  # Period 5: 2-3 pm\n",
    "    (15, 16)   # Period 6: 3-4 pm\n",
    "]\n",
    "TOTAL_PERIODS = 6\n",
    "MINIMUM_DURATION = 45  # in minutes to log attendance for the period\n",
    "\n",
    "# Load known faces and their encodings\n",
    "def load_known_faces():\n",
    "    known_faces = []\n",
    "    known_names = []\n",
    "\n",
    "    for name in os.listdir(KNOWN_FACES_DIR):\n",
    "        for filename in os.listdir(f\"{KNOWN_FACES_DIR}/{name}\"):\n",
    "            image = face_recognition.load_image_file(f\"{KNOWN_FACES_DIR}/{name}/{filename}\")\n",
    "            encodings = face_recognition.face_encodings(image)\n",
    "            if encodings:\n",
    "                known_faces.append(encodings[0])\n",
    "                known_names.append(name)\n",
    "    print(\"Loaded known faces.\")\n",
    "    return known_faces, known_names\n",
    "\n",
    "# Initialize or update attendance file with columns for each period and Total and Percentage\n",
    "def initialize_attendance_file(known_names):\n",
    "    attendance_file = r'C:\\Users\\vamshi\\OneDrive\\Desktop\\retina\\Attend.xlsx'\n",
    "    if not os.path.exists(attendance_file):\n",
    "        columns = ['Name'] + [f'Period {i+1}' for i in range(TOTAL_PERIODS)] + ['Total', 'Percentage']\n",
    "        attendance_data = pd.DataFrame(columns=columns)\n",
    "        attendance_data['Name'] = known_names\n",
    "        attendance_data.fillna(0, inplace=True)  # Set initial attendance to 0\n",
    "        attendance_data.to_excel(attendance_file, index=False)\n",
    "    else:\n",
    "        attendance_data = pd.read_excel(attendance_file, engine='openpyxl')\n",
    "    return attendance_data\n",
    "\n",
    "# Determine current period based on the current time\n",
    "def get_period_index(current_time):\n",
    "    for i, (start, end) in enumerate(PERIOD_TIMES):\n",
    "        if start <= current_time.hour < end:\n",
    "            return i  # Return the period index (0 for Period 1, etc.)\n",
    "    return None  # Return None if outside period times\n",
    "\n",
    "# Update attendance for each period based on entry and exit times\n",
    "def update_attendance(name, timestamp, event_type, attendance_data, last_entry_times):\n",
    "    attendance_file = r'C:\\Users\\vamshi\\OneDrive\\Desktop\\retina\\Attend.xlsx'\n",
    "    current_time = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "    period_index = get_period_index(current_time)\n",
    "    \n",
    "    if period_index is not None:\n",
    "        period_col = f'Period {period_index + 1}'\n",
    "        \n",
    "        if event_type == 'entry':\n",
    "            last_entry_times[name] = current_time  # Store entry time\n",
    "        elif event_type == 'exit' and name in last_entry_times:\n",
    "            entry_time = last_entry_times.pop(name, None)  # Retrieve and remove the entry time\n",
    "            duration = (current_time - entry_time).total_seconds() / 60  # Convert to minutes\n",
    "\n",
    "            # Log attendance only if the duration is at least 45 minutes\n",
    "            if duration >= MINIMUM_DURATION:\n",
    "                attendance_data.loc[attendance_data['Name'] == name, period_col] += 1\n",
    "\n",
    "                # Update total attendance count\n",
    "                attendance_data['Total'] = attendance_data[[f'Period {i + 1}' for i in range(TOTAL_PERIODS)]].sum(axis=1)\n",
    "\n",
    "                # Update percentage\n",
    "                total_attended = attendance_data.loc[attendance_data['Name'] == name, 'Total'].values[0]\n",
    "                attendance_data.loc[attendance_data['Name'] == name, 'Percentage'] = (total_attended / TOTAL_PERIODS) * 100\n",
    "\n",
    "                # Save updated attendance data to Excel\n",
    "                attendance_data.to_excel(attendance_file, index=False)\n",
    "                print(f\"Updated attendance for {name} for {period_col} (duration: {duration} mins)\")\n",
    "\n",
    "# Recognize faces from entry and exit cameras and process attendance\n",
    "def recognize_faces_multi_camera(known_faces, known_names):\n",
    "    cap_entry = cv2.VideoCapture(0)  # Entry camera\n",
    "    cap_exit = cv2.VideoCapture(1)   # Exit camera\n",
    "\n",
    "    attendance_data = initialize_attendance_file(known_names)\n",
    "    last_entry_times = {}  # Dictionary to track last entry times for each person\n",
    "\n",
    "    while True:\n",
    "        # Read frames from both entry and exit cameras\n",
    "        ret_entry, frame_entry = cap_entry.read()\n",
    "        ret_exit, frame_exit = cap_exit.read()\n",
    "        if not ret_entry or not ret_exit:\n",
    "            break\n",
    "\n",
    "        # Process entry camera\n",
    "        process_frame(frame_entry, 'entry', known_faces, known_names, attendance_data, last_entry_times)\n",
    "\n",
    "        # Process exit camera\n",
    "        process_frame(frame_exit, 'exit', known_faces, known_names, attendance_data, last_entry_times)\n",
    "\n",
    "        # Display frames for entry and exit cameras\n",
    "        cv2.imshow('Entry Camera', frame_entry)\n",
    "        cv2.imshow('Exit Camera', frame_exit)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap_entry.release()\n",
    "    cap_exit.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Process frames for entry or exit events\n",
    "def process_frame(frame, event_type, known_faces, known_names, attendance_data, last_entry_times):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    faces = RetinaFace.detect_faces(frame)\n",
    "    face_names = []\n",
    "\n",
    "    for face_key in faces.keys():\n",
    "        facial_area = faces[face_key][\"facial_area\"]\n",
    "        top, right, bottom, left = facial_area[1], facial_area[2], facial_area[3], facial_area[0]\n",
    "        encodings = face_recognition.face_encodings(frame_rgb, [(top, right, bottom, left)])\n",
    "\n",
    "        for encoding in encodings:\n",
    "            matches = face_recognition.compare_faces(known_faces, encoding, TOLERANCE)\n",
    "            name = \"Unknown\"\n",
    "            face_distances = face_recognition.face_distance(known_faces, encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            if matches[best_match_index]:\n",
    "                name = known_names[best_match_index]\n",
    "            face_names.append(name)\n",
    "\n",
    "    # Log detected names and timestamps\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    for name in face_names:\n",
    "        if name != \"Unknown\":\n",
    "            update_attendance(name, timestamp, event_type, attendance_data, last_entry_times)\n",
    "            print(f\"{event_type.capitalize()} detected for {name} at {timestamp}\")\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    known_faces, known_names = load_known_faces()\n",
    "    recognize_faces_multi_camera(known_faces, known_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vamshi\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Loaded known faces.\n",
      "Entry detected for vamshi at 2024-11-03 00:23:19\n",
      "Updated attendance for vamshi for Period 6 (duration: 2.03 mins)\n",
      "Exit detected for vamshi at 2024-11-03 00:25:21\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from retinaface import RetinaFace\n",
    "import face_recognition\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Directory containing known faces\n",
    "KNOWN_FACES_DIR = r\"C:\\Users\\vamshi\\OneDrive\\Desktop\\retina\\Training_images\"\n",
    "TOLERANCE = 0.6\n",
    "FRAME_THICKNESS = 3\n",
    "FONT_THICKNESS = 2\n",
    "MODEL = \"cnn\"\n",
    "\n",
    "# Constants for periods and attendance calculation\n",
    "PERIOD_TIMES = [\n",
    "    (9, 10),   # Period 1: 9-10 am\n",
    "    (10, 11),  # Period 2: 10-11 am\n",
    "    (11, 12),  # Period 3: 11-12 pm\n",
    "    (13, 14),  # Period 4: 1-2 pm\n",
    "    (14, 15),  # Period 5: 2-3 pm\n",
    "    (0, 1)   # Period 6: 3-4 pm\n",
    "]\n",
    "TOTAL_PERIODS = len(PERIOD_TIMES)\n",
    "MINIMUM_DURATION = 1  # Minimum duration in minutes to log attendance\n",
    "\n",
    "# Load known faces and their encodings\n",
    "def load_known_faces():\n",
    "    known_faces = []\n",
    "    known_names = []\n",
    "\n",
    "    for name in os.listdir(KNOWN_FACES_DIR):\n",
    "        for filename in os.listdir(f\"{KNOWN_FACES_DIR}/{name}\"):\n",
    "            image = face_recognition.load_image_file(f\"{KNOWN_FACES_DIR}/{name}/{filename}\")\n",
    "            encodings = face_recognition.face_encodings(image)\n",
    "            if encodings:\n",
    "                known_faces.append(encodings[0])\n",
    "                known_names.append(name)\n",
    "    print(\"Loaded known faces.\")\n",
    "    return known_faces, known_names\n",
    "\n",
    "  # Initialize or update attendance file with columns for each period and Total and Percentage\n",
    "def initialize_attendance_file(known_names):\n",
    "    attendance_file = r'C:\\Users\\vamshi\\OneDrive\\Desktop\\retina\\Attend.xlsx'\n",
    "    if not os.path.exists(attendance_file):\n",
    "        columns = ['Name', 'Date'] + [f'Period {i+1}' for i in range(TOTAL_PERIODS)] + ['Total', 'Percentage']\n",
    "        attendance_data = pd.DataFrame(columns=columns)\n",
    "        attendance_data['Name'] = pd.Series(known_names).unique() \n",
    "        attendance_data['Percentage'] = 100\n",
    "        attendance_data['Date'] = datetime.now().date()  # Set today's date\n",
    "        attendance_data.fillna(0, inplace=True)  # Set initial attendance to 0\n",
    "        attendance_data.to_excel(attendance_file, index=False)\n",
    "    else:\n",
    "        attendance_data = pd.read_excel(attendance_file, engine='openpyxl')\n",
    "    return attendance_data\n",
    "\n",
    "\n",
    "# Determine current period based on the current time\n",
    "def get_period_index(current_time):\n",
    "    for i, (start, end) in enumerate(PERIOD_TIMES):\n",
    "        if start <= current_time.hour < end:\n",
    "            return i  # Return the period index (0 for Period 1, etc.)\n",
    "    return None  # Return None if outside period times\n",
    "\n",
    "# Update attendance for each period based on entry and exit times\n",
    "# Update attendance for each period based on entry and exit times\n",
    "def update_attendance(name, timestamp, event_type, attendance_data, last_entry_times):\n",
    "    attendance_file = r'C:\\Users\\vamshi\\OneDrive\\Desktop\\retina\\Attend.xlsx'\n",
    "    current_time = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "    period_index = get_period_index(current_time)\n",
    "\n",
    "    if period_index is not None:\n",
    "        period_col = f'Period {period_index + 1}'\n",
    "\n",
    "        if event_type == 'entry':\n",
    "            last_entry_times[name] = current_time  # Store entry time\n",
    "        elif event_type == 'exit' and name in last_entry_times:\n",
    "            entry_time = last_entry_times.pop(name)  # Retrieve and remove the entry time\n",
    "            duration = (current_time - entry_time).total_seconds() / 60  # Convert to minutes\n",
    "\n",
    "            # Log attendance only if the duration is at least MINIMUM_DURATION\n",
    "            if duration >= MINIMUM_DURATION:\n",
    "                # Increment attendance for the current period\n",
    "                attendance_data.loc[attendance_data['Name'] == name, period_col] += 1\n",
    "                \n",
    "                # Update total attendance\n",
    "                current_total = attendance_data.loc[attendance_data['Name'] == name, 'Total'].iloc[0]\n",
    "                new_total = current_total + 1  # Increase total attendance count by 1\n",
    "                attendance_data.loc[attendance_data['Name'] == name, 'Total'] = new_total\n",
    "                \n",
    "                # Update total periods attended\n",
    "                total_periods_attended = attendance_data.loc[attendance_data['Name'] == name, 'Total'].iloc[0]\n",
    "                \n",
    "                # Update the total possible periods (this would need to consider the total number of periods over time)\n",
    "                # For this example, let's assume TOTAL_PERIODS is constant. If it changes, you need to adjust accordingly.\n",
    "                total_possible_periods = TOTAL_PERIODS * ((datetime.now().date() - attendance_data['Date'].min().date()).days+1)\n",
    "                    \n",
    "                # Calculate the attendance percentage\n",
    "                attendance_data.loc[attendance_data['Name'] == name, 'Percentage'] = (total_periods_attended / total_possible_periods) * 100\n",
    "                \n",
    "                # Save updated attendance data to Excel\n",
    "                attendance_data.to_excel(attendance_file, index=False)\n",
    "                print(f\"Updated attendance for {name} for {period_col} (duration: {duration:.2f} mins)\")\n",
    "\n",
    "\n",
    "\n",
    "# Recognize faces from entry and exit cameras and process attendance\n",
    "def recognize_faces_multi_camera(known_faces, known_names):\n",
    "    cap_entry = cv2.VideoCapture(0)  # Entry camera\n",
    "    cap_exit = cv2.VideoCapture(1)   # Exit camera\n",
    "\n",
    "    attendance_data = initialize_attendance_file(known_names)\n",
    "    last_entry_times = {}  # Dictionary to track last entry times for each person\n",
    "\n",
    "    while True:\n",
    "        # Read frames from both entry and exit cameras\n",
    "        ret_entry, frame_entry = cap_entry.read()\n",
    "        ret_exit, frame_exit = cap_exit.read()\n",
    "        if not ret_entry or not ret_exit:\n",
    "            break\n",
    "\n",
    "        # Process entry camera\n",
    "        process_frame(frame_entry, 'entry', known_faces, known_names, attendance_data, last_entry_times)\n",
    "\n",
    "        # Process exit camera\n",
    "        process_frame(frame_exit, 'exit', known_faces, known_names, attendance_data, last_entry_times)\n",
    "\n",
    "        # Display frames for entry and exit cameras\n",
    "        cv2.imshow('Entry Camera', frame_entry)\n",
    "        cv2.imshow('Exit Camera', frame_exit)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap_entry.release()\n",
    "    cap_exit.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Process frames for entry or exit events\n",
    "def process_frame(frame, event_type, known_faces, known_names, attendance_data, last_entry_times):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    faces = RetinaFace.detect_faces(frame)\n",
    "    face_names = []\n",
    "    for face_key in faces.keys():\n",
    "        facial_area = faces[face_key][\"facial_area\"]\n",
    "        top, right, bottom, left = facial_area[1], facial_area[2], facial_area[3], facial_area[0]\n",
    "        encodings = face_recognition.face_encodings(frame_rgb, [(top, right, bottom, left)])\n",
    "\n",
    "        for encoding in encodings:\n",
    "            matches = face_recognition.compare_faces(known_faces, encoding, TOLERANCE)\n",
    "            name = \"Unknown\"\n",
    "            face_distances = face_recognition.face_distance(known_faces, encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            if matches[best_match_index]:\n",
    "                name = known_names[best_match_index]\n",
    "            face_names.append(name)\n",
    "    # Log detected names and timestamps\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    for name in face_names:\n",
    "        if name != \"Unknown\":\n",
    "            update_attendance(name, timestamp, event_type, attendance_data, last_entry_times)\n",
    "            print(f\"{event_type.capitalize()} detected for {name} at {timestamp}\")\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    known_faces, known_names = load_known_faces()\n",
    "    recognize_faces_multi_camera(known_faces, known_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
